# export IMAGE="nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.4.0"
#export IMAGE="/lustre/fsw/coreai_tritoninference_triton3/rmccormick/images/dynamo-trtllm-0.4.0-amd64.sqsh"

# 1.0.0rc4
export IMAGE="/lustre/fsw/coreai_tritoninference_triton3/rmccormick/images/222e113ea3e58372b24d4b79f6c93a6ccaa4f9dd-33827263-trtllm-amd64.sqsh"

# 1.1.0rc1
#export IMAGE="/lustre/fsw/coreai_tritoninference_triton3/rmccormick/images/222e113ea3e58372b24d4b79f6c93a6ccaa4f9dd-33827291-trtllm-amd64.sqsh"

# Assumes $PWD is dynamo/components/backends/trtllm 
export MOUNTS="${PWD}:/mnt,/lustre:/lustre"

# JET path
export MODEL_PATH="/lustre/share/coreai_dlalgo_ci/artifacts/model/llama-4-maverick_17b_128e_pyt/safetensors_mode-instruct/hf-e91306a-dynamo-fp8/"
export SERVED_MODEL_NAME="nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8"

# DEBUG: Small model for quick sanity checks
# export MODEL_PATH="/lustre/share/coreai_dlalgo_ci/artifacts/model/qwen3_0.6b_pyt/safetensors_mode-instruct/hf-7765feb-nim-fp8/"
# export SERVED_MODEL_NAME="Qwen/Qwen3-0.6B"

# export ENGINE_CONFIG="/mnt/engine_configs/llama4/agg.yaml"
export PREFILL_ENGINE_CONFIG="/mnt/engine_configs/llama4/prefill.yaml"
export DECODE_ENGINE_CONFIG="/mnt/engine_configs/llama4/decode.yaml"

export NUM_GPUS_PER_NODE=8
export NUM_PREFILL_NODES=1
export NUM_DECODE_NODES=1

export NUM_PREFILL_WORKERS=4
export NUM_DECODE_WORKERS=2

export DISAGGREGATION_STRATEGY="prefill_first"

# FIXME
export MAX_NUM_TOKENS=40000
export PREFILL_MAX_BATCH_SIZE=2
export DECODE_MAX_BATCH_SIZE=4
